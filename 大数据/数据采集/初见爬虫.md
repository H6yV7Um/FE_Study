# 爬虫简介

## 一、网络连接

网络数据采集需要抛开一些接口的遮挡，不仅是在浏览器层(它如何解释所有的 HTML、CSS 和 JavaScript)，有时也包括网络连接层。

通过下面的例子让你对浏览器获取信息的过程有一个基本的认识。Alice 有一台网络 服务器。Bob 有一个台式机正准备连接 Alice 的服务器。当一台机器想与另一台机器对话 时，下面的某个行为将会发生。

1. Bob的电脑发送一串1和0比特值，表示电路上的高低电压。这些比特构成了一种信 息，包括请求头和消息体。请求头包含当前 Bob 的本地路由器 MAC 地址和 Alice 的 IP地址。消息体包含 Bob 对 Alice 服务器应用的请求。
2. Bob的本地路由器收到所有1和0比特值，把它们理解成一个数据包(packet)，从Bob
自己的 MAC 地址“寄到”Alice 的 IP 地址。他的路由器把数据包“盖上”自己的 IP 地址作为“发件”地址，然后通过互联网发出去。
3. Bob的数据包游历了一些中介服务器，沿着正确的物理/电路路径前进，到了Alice的
服务器。
4. Alice的服务器在她的IP地址收到了数据包。
5. Alice的服务器读取数据包请求头里的目标端口(通常是网络应用的80端口，可以理解
成数据包的“房间号”，IP 地址就是“街道地址”)，然后把它传递到对应的应用——网络服务器应用上。
6. 网络服务器应用从服务器处理器收到一串数据，数据是这样的:
 - 这是一个 GET 请求
 - 请求文件 index.html
7. 网络服务器应用找到对应的HTML文件，把它打包成一个新的数据包发送给Bob，然
后通过它的本地路由器发出去，用同样的过程回传到 Bob 的机器上。

## 二、可靠的网络连接

即使网页已经从服务器成功获取，如果网页上的内容并非完全是我们期望的那样， 仍然可能会出现异常。

```python
from urllib.request import urlopen 
from urllib.error import HTTPError 
from bs4 import BeautifulSoup

def getTitle(url):
	try:
		html = urlopen(url)
	except HTTPError as e:
		return None
		
	try:
		bsObj = BeautifulSoup(html.read())
		title = bsObj.body.h1
	except AttributeError as e:
		return None
	return title

title = geteTitle("http://www.baidu.com/pages/1.html")
if title == None:
	print("Title could not found")
else:
	print(title)
```

在这个例子中，创建了一个 getTitle 函数，可以返回网页的标题，如果获取网页 的时候遇到问题就返回一个 None 对象。在 getTitle 函数里面，检查了 HTTPError，然后把两行 BeautifulSoup 代码封装在一个 try 语句里面。这两行中的任何一 行有问题，AttributeError 都可能被抛出(如果服务器不存在，html 就是一个 None 对象， html.read() 就会抛出 AttributeError)。其实，可以在 try 语句里面放任意多行代码， 或者放一个在任意位置都可以抛出 AttributeError 的函数。

在写爬虫的时候，思考代码的总体格局，让代码既可以捕捉异常又容易阅读，这是很重要 的。如果你还希望能够很大程度地重用代码，那么拥有像 getSiteHTML 和 getTitle 这样的 通用函数(具有周密的异常处理功能)会让快速稳定地网络数据采集变得简单易行。
























