# 采集

## 一、遍历单个域名

“维基百科六度分隔理论”和“六度分隔值游戏”是把两个不相干的主题用一个总数不超过六条的主题连接起来(包括原来的两个主题)。

- 一个函数getLinks，可以用维基百科词条/wiki/<词条名称>形式的URL链接作为参数， 然后以同样的形式返回一个列表，里面包含所有的词条 URL 链接。
- 一个主函数，以某个起始词条为参数调用getLinks，再从返回的URL列表里随机选择 一个词条链接，再调用 getLinks，直到我们主动停止，或者在新的页面上没有词条链接 了，程序才停止运行。

```python
# -*- coding:utf-8 -*-
__author__ = 'Ben'

from urllib.request import urlopen
from bs4 import BeautifulSoup
import datetime
import random
import re

random.seed(datetime.datetime.now())

def getLinks(articleUrl):
    html = urlopen("http://en.wikipedia.org" + articleUrl)
    bsObj = BeautifulSoup(html)
    return bsObj.find("div", {"id":"bodyContent"}).findAll("a", href=re.compile("^(/wiki/)((?!:).)*$"))

links = getLinks("/wiki/Kevin_Bacon")
while len(links) > 0:
    newArticle = links[random.randint(0, len(links)-1)].attrs["href"]
    print(newArticle)
    links = getLinks(newArticle)
```

## 二、采集整个网站

需要系统地把整个网站按目录分类，或者要搜索网站上的每一个页面, 得采集整个网站，那是一种非常耗费内存资源的过程，尤其是处理大型网站时，最合适的工具就是用一个数据库来储存采集的资源。

什么时候采集整个网站是有用的，而什么时候采集整个网站又是有害无益的呢?遍 历整个网站的网络数据采集有许多好处。

- 生成网站地图

一个重要的客户想对一个网站的重新设计方案进行效 果评估，但是不想让公司进入他们的网站内容管理系统(CMS)，也没有一个公开 可用的网站地图。就用爬虫采集了整个网站，收集了所有的链接，再把所有的页面整 理成他们网站实际的形式。这很快找出了网站上以前不曾留意的部分，并准确地计 算出需要重新设计多少网页，以及可能需要移动多少内容。

- 收集数据

一个客户为了创建一个专业垂直领域的搜索平台，想收集一些文章(故事、博 文、新闻等)。虽然这些网站采集并不费劲，但是它们需要爬虫有足够的深度(有意收集数据的网站不多)。于是创建一个爬虫递归地遍历每个网站，只收集那些网站页面上的数据。

一个常用的费时的网站采集方法就是从顶级页面开始(比如主页)，然后搜索页面上的所有链接，形成列表。再去采集这些链接的每一个页面，然后把在每个页面上找到的链接形 成新的列表，重复执行下一轮采集。

很明显，这是一个复杂度增长很快的情形。假如每个页面有10个链接，网站上有5个页面深度(一个中等规模网站的主流深度)，那么如果要采集整个网站，一共得采集的网页数量就是105，即100000个页面。不过，虽然“5个页面深度，每页10个链接”是网站的主流配置，但其实很少有网站真的有100000甚至更多的页面。这是因为很大一部分 内链都是重复的。

为了避免一个页面被采集两次，链接去重是非常重要的。在代码运行时，把已发现的所有 链接都放到一起，并保存在方便查询的列表里(下文示例指 Python 的集合 set 类型)。只 有“新”链接才会被采集，之后再从页面中搜索其他链接:

```python
# -*- coding:utf-8 -*-
__author__ = 'Ben'

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

pages = set()

def getLinks(pageUrl):
    global pages
    html = urlopen("http://en.wikipedia.org"+pageUrl)
    bsObj = BeautifulSoup(html)
    for link in bsObj.findAll("a", href=re.compile("^(/wiki/)")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                # 我们遇到了新页面
                newPage = link.attrs['href']
                print(newPage)
                pages.add(newPage)
                getLinks(newPage)

getLinks("")
```

用 getLinks 处理一个空 URL，其实是维基百科的主页，因为在函数里空 URL 就 是 http://en.wikipedia.org。然后，遍历首页上每个链接，并检查是否已经在全局变量 集合 pages 里面了(已经采集的页面集合)。如果不在，就打印到屏幕上，并把链接加入 pages 集合，再用 getLinks 递归地处理这个链接。

### 收集整个网站数据

只是从一个页面跳到另一个页面，那么网络爬虫是非常无聊的。为了有效地使 用它们，在用爬虫的时候我们需要在页面上做些事情。让我们看看如何创建一个爬虫来收 集页面标题、正文的第一个段落，以及编辑页面的链接(如果有的话)这些信息。

往常一样，决定如何做好这些事情的第一步就是先观察网站上的一些页面，然后拟定一 个采集模式。通过观察几个维基百科页面，包括词条和非词条页面，比如隐私策略之类的 页面，就会得出下面的规则。

- 所有的标题(所有页面上，不论是词条页面、编辑历史页面还是其他页面)都是在h1 → span 标签里，而且页面上只有一个h1标签。

- 所有的正文文字都在div#bodyContent标签里。但是，如果我们想更进一步获取第一段文字，可能用 div#mw-content-text → p 更好(只选择第一段的标 签)。这个规则对所有页面都适用，除了文件页面(例如，https://en.wikipedia.org/wiki/ File:Orbit_of_274301_Wikipedia.svg)，页面不包含内容文字(content text)的部分内容。

- 编辑链接只出现在词条页面上。如果有编辑链接，都位于li#ca-edit标签的li#ca- edit→span→a里面。

建立一个爬虫和数据收集(至少是数据打印)的组合程序:

```python

# -*- coding:utf-8 -*-
__author__ = 'Ben'

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

pages = set()

def getLinks(pageUrl):
    global pages
    html = urlopen("http://en.wikipedia.org"+pageUrl)
    bsObj = BeautifulSoup(html)
    try:
        print(bsObj.h1.get_text())
        print(bsObj.find(id="mw-content-text").findAll("p")[0])
        print(bsObj.find(id="ca-edit").find("span").find("a").attrs['href'])
    except AttributeError:
        print("页面缺少一些属性!不过不用担心!")

    for link in bsObj.findAll("a", href=re.compile("^(/wiki/)")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                # 我们遇到了新页面
                newPage = link.attrs['href']
                print("----------------\n"+newPage)
                pages.add(newPage)
                getLinks(newPage)
getLinks("")

```

因为不可能确保每一页上都有所有类型的数据，所以每个打印语句都是按照数据在页面上出现的可能性从高到低排列的。也就是说，h1标题标签会出现在每一页上(只要能识别，无论哪一页都有)，所以首先试着获取它的数据。正文内容会出现在大多数页面上(除了文件页面)，因此是第二个获取的数据。“编辑”按钮只出现在标题和正文内容都已经获取的页面上，但不是所有这类页面上都有，所以最后打印这类数据。

## 三、通过互联网采集

在写爬虫随意跟随外链跳转之前，请问自己几个问题。

- 我要收集哪些数据?这些数据可以通过采集几个已经确定的网站(永远是最简单的做法) 完成吗?或者我的爬虫需要发现那些我可能不知道的网站吗?

- 当我的爬虫到了某个网站，它是立即顺着下一个出站链接跳到一个新网站，还是在网站 上呆一会儿，深入采集网站的内容?

- 有没有我不想采集的一类网站?我对非英文网站的内容感兴趣吗?

- 如果我的网络爬虫引起了某个网站网管的怀疑，我如何避免法律责任?



















